import requests
import json
import random
import copy
import pprint
import torch
from PIL import Image

import sys
import warnings
import os

from utils import load_and_preprocess_dataset, setup_models, deduplicate_qars, setup_final_judge, get_gpu_details

def judge_qars(qars, image, judge_mllm):
    qars = judge_mllm.evaluate(qars, image)
    return qars

def verify_inference(qars, image, br_mllm):
    qars = br_mllm.verify_inference(qars, image)
    return qars

# TODO: Later, make parallel inference calls (as possible)
def generate_qars(dataset, generator_models, judge_model, br_model):
    synthetic_qars = []
    # Load aokvqa/scienceqa dataset
    # Step 0: Load and preprocess dataset
    data = load_and_preprocess_dataset(dataset)
    random_i = random.randint(0, len(data)-1)
    image = data[random_i]["image"]
    #image = data[15572]["image"]
    #image = data[9344]["image"]
    
    #print(random_i)
    
    # Step 1: Load models
    # TODO: Add more generator mllms (i.e. llava(done), molmo)
    generator_mllms, judge_mllm, br_mllm = setup_models(generator_models, judge_model, br_model)
    final_judge_mllm = setup_final_judge()
    
    # Remove after testing
    #url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg"
    #image = Image.open(requests.get(url, stream=True).raw)
    
    evolvable_questions = []
    tries = 0
    # LATER: Make 3 tries
    while tries < 1:
        # TODO: Check how mllms (existing ones perform in evolving task) 
        # Step 2: Generate qars
        #query = "Can you describe the activity of the animal in context of the image?"
        #query = "Can you generate 3 non-trivial, diverse questions and corresponding answers based on the image without hallucinating? Keep the answers precise and short (no over explanation).Return the question-answer pairs in a list following this structure: [{'question': <question>, 'answer': <answer>}]. Return only the list of JSON objects, nothing else."
        
        use_evol_prompt = False
        if len(evolvable_questions) != 0:
            use_evol_prompt = True

        all_syn_qars = {}
        # TODO: Test with >1 mllm (i.e. llama 3.2, llava, molmo)
        for i, mllm in enumerate(generator_mllms):
            questions = mllm.generate(image, use_evol_prompt, questions=None, evolvable_questions=evolvable_questions)
            syn_qars = mllm.generate(image, use_evol_prompt=False, questions=questions, evolvable_questions=[])
            try:
                syn_qars = json.loads(syn_qars)
            except json.JSONDecodeError:
                print(f'Error: Could not parse syn_qars for {random_i}-th training image, moving to next mllm.')
                continue
            
            all_syn_qars[f'mllm_{i+1}'] = {}
            for j, qar in enumerate(syn_qars):
                all_syn_qars[f'mllm_{i+1}'][f'qar_{j+1}'] = qar
                    
        # Step 3: Judge (soft filter) qars generated by each generator mllm
        for mllm in all_syn_qars:
            judged_qars = []
            syn_qars = []
            for qar in all_syn_qars[mllm]:
                syn_qars.append(all_syn_qars[mllm][qar])

            judged_qars = judge_qars(syn_qars, image, judge_mllm)

            i = 0
            for qar in all_syn_qars[mllm]:
                all_syn_qars[mllm][qar] = judged_qars[i]
                i += 1
        
        # Step 4: Do inference verification using backward reasoner mllm
        for mllm in all_syn_qars:
            inference_verified_qars = []
            judged_syn_qars = []
            for qar in all_syn_qars[mllm]:
                judged_syn_qars.append(all_syn_qars[mllm][qar])
            inference_verified_qars = verify_inference(judged_syn_qars, image, br_mllm)

            i = 0
            for qar in all_syn_qars[mllm]:
                all_syn_qars[mllm][qar] = inference_verified_qars[i]
                i += 1

        # step 5: keep all filtered in quality qars
        syn_qar_bucket = []
        for mllm in all_syn_qars:
            for qar in all_syn_qars[mllm]:
                if (all_syn_qars[mllm][qar]['br_score'] > 0.7):
                    syn_qar_bucket.append(all_syn_qars[mllm][qar])
        
        # step 6: de-duplicate initially filtered qars
        unique_qars = deduplicate_qars(syn_qar_bucket)
        
        evolvable_questions = []
        unique_qars = final_judge_mllm.evaluate(unique_qars, image)
        for unique_qar in unique_qars:
            # Current qar can be stored in final dataset
            if unique_qar["evaluation"]["score"] >= 80:
                # TODO: Generate options (use phi3-mini like Moshiur did)
                synthetic_qars.append({
                    "question": unique_qar["question"],
                    "answer": unique_qar["answer"],
                    "rationale": unique_qar["rationale"]
                })

            # This additional check is done, since the score is not always reliable (arithmetic mistakes may occur by MLLM)
            # Check if current qar can be further evolved
            failures = unique_qars["evaluation"]["failures"]
            evol_text = unique_qar["evaluation"]["evolution_method"]
            if  evol_text and (('None' not in failures) or (evol_text != 'None') or (evol_text is not None)):
                evolvable_questions.append({
                    "question": unique_qar["question"],
                    "evolution_inst": unique_qar["evaluation"]["evolution_method"]
                })

        # All qars are evolved as much as possible
        if len(evolvable_questions) == 0:
            break

        #pprint.pprint(evolvable_questions)
        
        tries += 1
    
    #TODO: Apply dedup again on final synthetic dataset

    #TODO: Store in huggingface



     