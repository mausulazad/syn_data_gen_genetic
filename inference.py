import requests
import json
import random
import copy
import pprint
import torch
from PIL import Image

import sys
import warnings
import os

from utils import load_and_preprocess_dataset, setup_slm, setup_models, postprocess_qars, deduplicate_qars, setup_final_judge, get_gpu_details, load_json_file, generate_sample_data

def judge_qars(qars, image, judge_mllm):
    qars = judge_mllm.evaluate(qars, image)
    return qars

def verify_inference(qars, image, br_mllm):
    qars = br_mllm.verify_inference(qars, image)
    return qars

# TODO: Later, make parallel inference calls (as possible)
def generate_qars(dataset, generator_models, judge_model, br_model):
    synthetic_qars = []
    
    # Load aokvqa/scienceqa dataset
    # Step 0: Load and preprocess dataset
    image_data = load_and_preprocess_dataset(dataset)
    #random_i = random.randint(0, len(image_data)-1)
    #image = image_data[random_i]["image"]
    #image = image_data[15572]["image"]
    #image = image_data[9344]["image"]

    # Step 1: Load models
    # TODO: Add more generator mllms (i.e. llava(done), molmo)
    # Setup a SLM (Llama-3.2 1B/3B) for output structure related post-processing
    slm = setup_slm()
    generator_mllms, judge_mllm, br_mllm = setup_models(generator_models, judge_model, br_model)
    #final_judge_mllm = setup_final_judge()
    

    """
    for i, qar_text in enumerate(qar_texts):
        qars = postprocess_qars(slm, qar_text)
        print(f"\nSAMPLE {i+1}: ")
        try:
            syn_qars = json.loads(qars)
        except json.JSONDecodeError:
            #print(f'Error: Could not parse syn_qars for {random_i}-th training image, moving to next mllm.')
            continue
        print(type(syn_qars))
        if len(syn_qars) != 0:
            print(type(syn_qars[0]))
        print("xxxxxxxxxxxxxxxxxxxxxxxx")
    """    
    
    """
    sample_syn_qars = load_json_file(file_name='sample_q_no_rules_zero_shot.json') 
    
    image_ids = []
    for syn_qar in sample_syn_qars:
        image_ids.append(syn_qar["serial"])

    sample_syn_qars = generate_sample_data(image_data, 'sample_q_few_shot_cot_molmo.json', slm, generator_mllms[0], image_ids)      
    print(len(sample_syn_qars))
    """

    #print(random_i)
    # Remove after testing  
    #url = "https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg"
    #image = Image.open(requests.get(url, stream=True).raw)

    '''
    for idx in image_ids:
        image = image_data[idx]["image"]
    '''
    
    '''
    # TODO: Place this entire commneted block inside the for loop
    evolvable_questions = []
    tries = 0
    # LATER: Make 3 tries
    while tries < 1:
        # TODO: Check how mllms (existing ones perform in evolving task) 
        # Step 2: Generate qars
        #query = "Can you describe the activity of the animal in context of the image?"
        #query = "Can you generate 3 non-trivial, diverse questions and corresponding answers based on the image without hallucinating? Keep the answers precise and short (no over explanation).Return the question-answer pairs in a list following this structure: [{'question': <question>, 'answer': <answer>}]. Return only the list of JSON objects, nothing else."
        
        use_evol_prompt = False
        if len(evolvable_questions) != 0:
            use_evol_prompt = True

        all_syn_qars = {}
        # TODO: Test with >1 mllm (i.e. llama 3.2, llava, molmo)
        for i, mllm in enumerate(generator_mllms):
            questions = mllm.generate(image, use_evol_prompt, questions=None, evolvable_questions=evolvable_questions)
            syn_qars = mllm.generate(image, use_evol_prompt=False, questions=questions, evolvable_questions=[])
            syn_qars = postprocess_qars(syn_qars)
            try:
                syn_qars = json.loads(syn_qars)
            except json.JSONDecodeError:
                print(f'Error: Could not parse syn_qars for {random_i}-th training image, moving to next mllm.')
                continue
            
            all_syn_qars[f'mllm_{i+1}'] = {}
            for j, qar in enumerate(syn_qars):
                all_syn_qars[f'mllm_{i+1}'][f'qar_{j+1}'] = qar
                    
        # Step 3: Judge (soft filter) qars generated by each generator mllm
        for mllm in all_syn_qars:
            judged_qars = []
            syn_qars = []
            for qar in all_syn_qars[mllm]:
                syn_qars.append(all_syn_qars[mllm][qar])

            judged_qars = judge_qars(syn_qars, image, judge_mllm)

            i = 0
            for qar in all_syn_qars[mllm]:
                all_syn_qars[mllm][qar] = judged_qars[i]
                i += 1
        
        # Step 4: Do inference verification using backward reasoner mllm
        for mllm in all_syn_qars:
            inference_verified_qars = []
            judged_syn_qars = []
            for qar in all_syn_qars[mllm]:
                judged_syn_qars.append(all_syn_qars[mllm][qar])
            inference_verified_qars = verify_inference(judged_syn_qars, image, br_mllm)

            i = 0
            for qar in all_syn_qars[mllm]:
                all_syn_qars[mllm][qar] = inference_verified_qars[i]
                i += 1

        # step 5: keep all filtered in quality qars
        syn_qar_bucket = []
        for mllm in all_syn_qars:
            for qar in all_syn_qars[mllm]:
                if (all_syn_qars[mllm][qar]['br_score'] > 0.7):
                    syn_qar_bucket.append(all_syn_qars[mllm][qar])
        
        # step 6: de-duplicate initially filtered qars
        unique_qars = deduplicate_qars(syn_qar_bucket)
        
        evolvable_questions = []
        unique_qars = final_judge_mllm.evaluate(unique_qars, image)
        for unique_qar in unique_qars:
            # Current qar can be stored in final dataset
            if unique_qar["evaluation"]["score"] >= 80:
                # TODO: Generate options (use phi3-mini like Moshiur did)
                synthetic_qars.append({
                    "question": unique_qar["question"],
                    "answer": unique_qar["answer"],
                    "rationale": unique_qar["rationale"]
                })

            # This additional check is done, since the score is not always reliable (arithmetic mistakes may occur by MLLM)
            # Check if current qar can be further evolved
            failures = unique_qars["evaluation"]["failures"]
            evol_text = unique_qar["evaluation"]["evolution_method"]
            if  evol_text and (('None' not in failures) or (evol_text != 'None') or (evol_text is not None)):
                evolvable_questions.append({
                    "question": unique_qar["question"],
                    "evolution_inst": unique_qar["evaluation"]["evolution_method"]
                })

        # All qars are evolved as much as possible
        if len(evolvable_questions) == 0:
            break

        #pprint.pprint(evolvable_questions)
        
        tries += 1
    '''
    
    #TODO: Apply dedup again on final synthetic dataset

    #TODO: Store in huggingface



     